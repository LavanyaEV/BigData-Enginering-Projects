### Introduction
This is related to the data movement of CDC events related to Product Sales data, capturing it and having it in the Datalake (S3). On top of S3, we will be doing analysis of the data. This will be a near real-time processing. DynamoDB stream will publish CDC events. Kinesis stream will consume cdc events. Kinesis firehose will batch the data in near-real time and will dump it to s3 (batching is needed becoz it does not make sense to put each event row by row to s3, so it will create a small batch). Athena is used for Data Analysis. We will connect DynamoDb and Kinesis using eventbringde pipe. To produce data, we will use python code to generate mock data. DynamoDb is a NOSQL in AWS. 

### Pre-Requisites
- AWS Free-tier account
- Basic to Medium level understanding of various AWS services
- Basic to Medium Python Knowledge
- ETL knowledge

### AWS Services Used
- Lambda Function
- S3
- DynamoDB, DynamoDB Stream
- Kinesis Data Stream, Kinesis Firehose
- Glue Crawlers
- EventBridge Pipe
- Athena

### Architecture Flow
![image](https://github.com/LavanyaEV/BigData-Enginering-Projects/assets/48172931/3769642f-b1a8-4bf6-806f-fde029f399be)
Mock data generated by Lambda will be published to DynamoDB/DynamoDB stream. Then we will have continuously running Eventbridge Pipe. Then data will be consumed by Kinesis stream. Then, Kinesis Firehose will batch the data. From Kinesis Firehose, we can use lambda to perform some small transformation on each batch of records returned by Firehose. Then these transformed records are dumped into S3. From there crawler will crawl S3 bucket and then in Athena, we can do Analysis.

