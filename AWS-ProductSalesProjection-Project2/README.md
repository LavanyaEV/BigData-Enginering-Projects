### Introduction
This is related to the data movement of CDC events related to Product Sales data, capturing it and having it in the Datalake (S3). On top of S3, we will be doing analysis of the data. This will be a near real-time processing. DynamoDB stream will publish CDC events. Kinesis stream will consume cdc events. Kinesis firehose will batch the data in near-real time and will dump it to s3 (batching is needed becoz it does not make sense to put each event row by row to s3, so it will create a small batch). Athena is used for Data Analysis. We will connect DynamoDb and Kinesis using eventbringde pipe. To produce data, we will use python code to generate mock data. DynamoDb is a NOSQL in AWS. 

### Pre-Requisites
- AWS Free-tier account
- Basic to Medium level understanding of various AWS services
- Basic to Medium Python Knowledge
- ETL knowledge

### AWS Services Used
- Lambda Function
- S3
- DynamoDB, DynamoDB Stream
- Kinesis Data Stream, Kinesis Firehose
- Glue Crawlers
- EventBridge Pipe
- Athena

### Architecture Flow
<img width="400" alt="Architecture" src="https://github.com/LavanyaEV/BigData-Enginering-Projects/assets/48172931/3769642f-b1a8-4bf6-806f-fde029f399be">

Mock data generated by Lambda will be published to DynamoDB/DynamoDB stream. Then we will have continuously running Eventbridge Pipe. Then data will be consumed by Kinesis stream. Then, Kinesis Firehose will batch the data. From Kinesis Firehose, we can use lambda to perform some small transformation on each batch of records returned by Firehose. Then these transformed records are dumped into S3. From there crawler will crawl S3 bucket and then in Athena, we can do Analysis.

### Step-by-Step Explanation
- First create DynamoDB table with partition key as orderid.
- Then in order to capture cdc events (as data can come back again for the same key), we have to turn on DynamoDb Stream. Turn on Dynamodb stream with view type as New Image.
<img width="400" alt="Dynamodb" src="https://github.com/LavanyaEV/BigData-Enginering-Projects/assets/48172931/7b1ee3d5-5ef9-46d5-97e0-9984e303024f">
<img width="400" alt="Stream" src="https://github.com/LavanyaEV/BigData-Enginering-Projects/assets/48172931/e661c1ee-46af-4d13-b20a-cdec42a5ecf8">

- Now setup the kinesis stream. Provide name and keep rest of the things default. You can see in below image, by default it will create shards. It follows the concept of sharding to store data in a distributed manner coming from the stream.
<img width="400" alt="Kinesis" src="https://github.com/LavanyaEV/BigData-Enginering-Projects/assets/48172931/822d925f-5efd-42ff-9273-0069747ef6a4">
<img width="400" alt="Kinesis" src="https://github.com/LavanyaEV/BigData-Enginering-Projects/assets/48172931/29a2fbe8-07c1-46a0-887b-4dd4288914eb">


