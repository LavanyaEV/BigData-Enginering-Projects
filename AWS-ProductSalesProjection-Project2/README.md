### Introduction
This is related to the data movement of CDC events related to Product Sales data, capturing it and having it in the Datalake (S3). On top of S3, we will be doing analysis of the data. This will be a near real-time processing. DynamoDB stream will publish CDC events. Kinesis stream will consume cdc events. Kinesis firehose will batch the data in near-real time and will dump it to s3 (batching is needed becoz it does not make sense to put each event row by row to s3, so it will create a small batch). Athena is used for Data Analysis. We will connect DynamoDb and Kinesis using eventbringde pipe. To produce data, we will use python code to generate mock data. DynamoDb is a NOSQL in AWS. 

### Pre-Requisites
- AWS Free-tier account
- Basic to Medium level understanding of various AWS services
- Basic to Medium Python Knowledge
- ETL knowledge

### AWS Services Used
- Lambda Function
- S3
- DynamoDB, DynamoDB Stream
- Kinesis Data Stream, Kinesis Firehose
- Glue Crawlers
- EventBridge Pipe
- Athena

### Architecture Flow
<img width="400" alt="Architecture" src="https://github.com/LavanyaEV/BigData-Enginering-Projects/assets/48172931/3769642f-b1a8-4bf6-806f-fde029f399be">

Mock data generated by Lambda will be published to DynamoDB/DynamoDB stream. Then we will have continuously running Eventbridge Pipe. Then data will be consumed by Kinesis stream. Then, Kinesis Firehose will batch the data. From Kinesis Firehose, we can use lambda to perform some small transformation on each batch of records returned by Firehose. Then these transformed records are dumped into S3. From there crawler will crawl S3 bucket and then in Athena, we can do Analysis.

### Step-by-Step Explanation
- First create DynamoDB table with partition key as orderid.
- Then in order to capture cdc events (as data can come back again for the same key), we have to turn on DynamoDb Stream. Turn on Dynamodb stream with view type as New Image.
<img width="400" alt="Dynamodb" src="https://github.com/LavanyaEV/BigData-Enginering-Projects/assets/48172931/7b1ee3d5-5ef9-46d5-97e0-9984e303024f">
<img width="400" alt="Stream" src="https://github.com/LavanyaEV/BigData-Enginering-Projects/assets/48172931/e661c1ee-46af-4d13-b20a-cdec42a5ecf8">

- Now setup the kinesis stream. Provide name and keep rest of the things default. You can see in below image, by default it will create shards. It follows the concept of sharding to store data in a distributed manner coming from the stream.
<img width="400" alt="Kinesis" src="https://github.com/LavanyaEV/BigData-Enginering-Projects/assets/48172931/822d925f-5efd-42ff-9273-0069747ef6a4">
<img width="400" alt="Kinesis" src="https://github.com/LavanyaEV/BigData-Enginering-Projects/assets/48172931/29a2fbe8-07c1-46a0-887b-4dd4288914eb">

- Now understand producer code in this repo. You can either run the code from your terminal. For that, first you have to configure AWS properly from your terminal in order to establish connection with AWS services. Else, you can create a lambda function with python environment to insert data in DynamoDb, paste the same code with some minor changes and deploy it, click on test. In permissions, attach DynamoDB full access policy.
<img width="400" alt="Terminal" src="https://github.com/LavanyaEV/BigData-Enginering-Projects/assets/48172931/84ab783f-b9f2-4ea5-80dd-590b7f64cc3d">
<img width="400" alt="Lambda" src="https://github.com/LavanyaEV/BigData-Enginering-Projects/assets/48172931/3b025a62-7e10-4f60-8c4a-2f697a3d3242">
<img width="400" alt="lambda" src="https://github.com/LavanyaEV/BigData-Enginering-Projects/assets/48172931/93ecd80a-60ec-46ad-a1c4-9bad77a7470c">
<img width="400" alt="lambda" src="https://github.com/LavanyaEV/BigData-Enginering-Projects/assets/48172931/664dfd25-7bc2-47b6-9b47-ec21164a316f">

- Open cloudwatch logs from lambda function, you can see the logs generated. Now open the OrderTable in DynamoDB. Click on Explore Table Items option above and we can see all the ingested data by producer code.
<img width="400" alt="logs" src="https://github.com/LavanyaEV/BigData-Enginering-Projects/assets/48172931/4fb5bfe8-2a93-43f0-8f18-08e6f93f96c5">
<img width="400" alt="dynamodb" src="https://github.com/LavanyaEV/BigData-Enginering-Projects/assets/48172931/8cdf12ca-4dc8-4208-906d-d6ecce216930">

- Next create Eventbridge Pipe for point-to-point integration.  Give source as DynamoDB and select the OrderTable stream. Then select starting position as Latest (position in the stream to start reading from). Keep other options default (u can also change batch size in Additional settings). You can remove Filtering and enrichment option. In target, select Kinesis stream. Select stream that we created above. Give partition key as eventID. Its because, the event generated by DynamoDB stream will have eventid, and in that way we want event to get distributed (this eventid will decide which particular shard the data will go to). Then click create pipe.
<img width="400" alt="eventpipe" src="https://github.com/LavanyaEV/BigData-Enginering-Projects/assets/48172931/b9a86fca-40b0-4f26-81ee-24923e23c57c">
<img width="400" alt="eventpipe" src="https://github.com/LavanyaEV/BigData-Enginering-Projects/assets/48172931/b8be2cfb-344b-4487-bb3b-a79e21031099">

- Go to the pipe and in permission, attach 2 more policies, Dynamodb Full Access and Kinesis Full Access.
<img width="400" alt="eventpipe" src="https://github.com/LavanyaEV/BigData-Enginering-Projects/assets/48172931/43a3c12f-f206-4a49-a0d2-da96ea54869d">

- Now, run the producer code. Check in the kinesis stream, it will have started consuming the data. You can see the eventID generated, and the values inside it also. Everything will be in string format. This will not take care of the data types. That we will handle in transformation layer of lambda.
<img width="400" alt="kinesisstream" src="https://github.com/LavanyaEV/BigData-Enginering-Projects/assets/48172931/4345b3ad-3d3a-4c94-8320-e7592a74a00c">
<img width="400" alt="kinesisstream" src="https://github.com/LavanyaEV/BigData-Enginering-Projects/assets/48172931/0aa73dae-9448-47a9-9141-a09e4ec9466e">

- Now we have to create Kinesis Firehose. We cannot dump each and every record generated by kinesis stream one-by-one to s3. So, firehose will help to batch records from the stream and put it to S3. Create an S3 bucket to hold the output data from Firehose. Before dumping the data from Firehose to S3, we will do a small transformation using lambda (there is an option to tick in Firehose for this to happen). For this, create a new lambda function and paste the code as given in repo. It will have recordID (records batched by firehose will have recordid). This code will change data type of records in each batch from firehose. Also attach policies, s3access, dynamodb access, kinesis access, lambdakinesis access, firehose access.
<img width="400" alt="lambda" src="https://github.com/LavanyaEV/BigData-Enginering-Projects/assets/48172931/f132ffdf-2f1a-41e5-a010-21277a5c1795">

- Create a Firehose stream, give name, select source and destination as below. Provide kinesis data stream. Then tick Turn on Data Transformation and select the lambda we created above to do transformation. In destination settings, provide S3 bucket we created. Keep others default and click on create. After getting created, open that stream and attach policies to have access.
<img width="400" alt="firehose" src="https://github.com/LavanyaEV/BigData-Enginering-Projects/assets/48172931/9737dc48-7392-4c5b-a5d2-834612461b01">
<img width="400" alt="firehose" src="https://github.com/LavanyaEV/BigData-Enginering-Projects/assets/48172931/63f2a851-b700-483f-9a26-c978b76ac76c">
<img width="400" alt="firehose" src="https://github.com/LavanyaEV/BigData-Enginering-Projects/assets/48172931/712406d5-6ee1-448d-b533-d64c339997f3">
<img width="400" alt="firehose" src="https://github.com/LavanyaEV/BigData-Enginering-Projects/assets/48172931/504d4a80-09a5-4517-87f3-ed69eab6e7e7">

- Now again start publishing the data (as it will only take the latest data, since we configured it like that). If you check the cloudwatch logs of Transformation Lambda function that we created, you can see the below logs with recordID, rows etc.
<img width="400" alt="logs" src="https://github.com/LavanyaEV/BigData-Enginering-Projects/assets/48172931/7035bd05-4add-4354-90ea-cc0140143462">
<img width="400" alt="logs" src="https://github.com/LavanyaEV/BigData-Enginering-Projects/assets/48172931/6da2a641-d89d-4b92-8944-dd1d761788d7">

- Now, check in S3 whether we received our stream of data from firhose. This will take some time to get dumped in S3. You can see the data being ingested, can download the data and check it.
<img width="400" alt="s3" src="https://github.com/LavanyaEV/BigData-Enginering-Projects/assets/48172931/7d1a08ae-262f-42bc-9814-d911caf697eb">
<img width="400" alt="s3" src="https://github.com/LavanyaEV/BigData-Enginering-Projects/assets/48172931/4a450788-a8b2-41f4-bcea-40a339c0ccb8">

- Next we have to create a crawler on top of this s3 bucket. Go to Glue, create a database inside it. Click create crawler. Give name, also give data source as S3. Now, since our data in S3 is in json format, we have to add custom classifier. Choose json and give the json path (schema of data with $.). Then provide IAM role for crawler and in output, select the database and click create crawler. Now run the crawler.
<img width="400" alt="crawler" src="https://github.com/LavanyaEV/BigData-Enginering-Projects/assets/48172931/9537fc0f-8d65-4471-9e58-561a03fedb97">
<img width="400" alt="crawler" src="https://github.com/LavanyaEV/BigData-Enginering-Projects/assets/48172931/d6b7d364-95a0-4562-a442-1be293a25afe">
<img width="400" alt="crawler" src="https://github.com/LavanyaEV/BigData-Enginering-Projects/assets/48172931/21096a68-57be-4c55-bec8-f8bd733f3f52">

- Check the database table for crawler results. We can see the column names and 4 partition (its becoz we had sub-folders inside s3 bucket 2024/30/11/datafile). So it represents year, month, date, then data.
<img width="400" alt="crawler" src="https://github.com/LavanyaEV/BigData-Enginering-Projects/assets/48172931/a088a194-8c01-4666-8ca7-07b3c5e6e09a">
<img width="400" alt="crawler" src="https://github.com/LavanyaEV/BigData-Enginering-Projects/assets/48172931/9c5e3869-ee4f-42dc-8699-496aa478c3ac">

- Now go to Athena and query for the above table. You can see the results. This is a real-time pipeline. So in order to get the updated data always, schedule the crawler, so that it will read all the new partitions and will update the data, so that we can get updated data in Athena. Now we can do Analysis on top of this data, can find projected sales, profit etc.
<img width="400" alt="athena" src="https://github.com/LavanyaEV/BigData-Enginering-Projects/assets/48172931/f2b443ba-a295-4bd5-b6a7-00d474c9f423">
<img width="400" alt="athena" src="https://github.com/LavanyaEV/BigData-Enginering-Projects/assets/48172931/6506d0a9-d596-4de4-b9d1-5e90fc9d3428">

Congrats, you completed the project!!!
































