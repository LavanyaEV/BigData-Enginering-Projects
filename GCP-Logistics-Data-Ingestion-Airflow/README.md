### Introduction
This project is related to Logistics Data Ingestion. Here, we will daily receive a .csv file having logistics data, which contains delivery_id, destination, delivery_status, vehicle type etc. We will use Hive as our Data warehousing service and source is Google cloud Bucket. Target table should be partitioned. Also, target table ingestion should start as soon as the file arrives. 

### Pre-Requisites
- GCP Free-tier account
- Airflow Knowledge
- Knowledge of Hive/Hadoop Commands
- Basic to Medium Python Knowledge

### Tech Stacks Used
- GCP Bucket
- Composer (Managed Airflow)
- DataProc Cluster
- Hive Commands
- Python

### Architecture
<img width="400" alt="Architecture" src="https://github.com/LavanyaEV/BigData-Enginering-Projects/assets/48172931/37b741d4-f14f-46a1-a563-541cde326e67">


